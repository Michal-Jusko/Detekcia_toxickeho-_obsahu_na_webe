{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"71886f57-90aa-4742-bf30-a1ad0c51d479","cell_type":"markdown","source":"# 1. Inštalácia knižníc a prostredia","metadata":{}},{"id":"7c672f1b","cell_type":"code","source":"# Trainer nepracuje správne v novšej verzii \n#Úplné odstránenie nekompatibilných verzií\n!pip uninstall -y transformers huggingface-hub tokenizers sentence-transformers peft\n\n# Vyčistenie cache\n!rm -rf /root/.cache/huggingface\n\n#Inštalácia stabilných verzií\n!pip install transformers==4.40.1 huggingface-hub==0.30.2 tokenizers==0.19.1\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"id":"27b24806-82c8-4663-b465-ca0edc93c303","cell_type":"markdown","source":"# 2. Príprava dát a tokenizácia","metadata":{}},{"id":"b5133528-fe71-468a-96c8-997b235589d7","cell_type":"code","source":"\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndataset = load_dataset(\"civil_comments\", split=\"train\")\ndf = dataset.to_pandas()\ndf[\"toxicity_label\"] = (df[\"toxicity\"] >= 0.5).astype(int)\n\n# Stratifikovaný výber 20 000 komentárov\ndf_sample, _ = train_test_split(df, train_size=20000, stratify=df[\"toxicity_label\"], random_state=42)\n\n# Stratifikované rozdelenie train val test\ntrain_df, temp_df = train_test_split(df_sample, test_size=0.2, stratify=df_sample[\"toxicity_label\"], random_state=42) # test_size=0.3, ale nerovnováha tried bola väčšia\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"toxicity_label\"], random_state=42)\n\n# Výpis \nfor name, split in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n    print(f\"{name} množina: {len(split)} komentárov\")\n    print(split[\"toxicity_label\"].value_counts(normalize=True).rename(\"proportion\"))\n    print(split[\"toxicity_label\"].value_counts().rename(\"počet\"), \"\\n\")\n\ntrain_df.to_csv(\"train_civil.csv\", index=False)\nval_df.to_csv(\"val_civil.csv\", index=False)\ntest_df.to_csv(\"test_civil.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0004e6f7-8a6b-4247-bc43-193c00fd4473","cell_type":"markdown","source":"# 2. Import knižníc","metadata":{}},{"id":"d2c7c642","cell_type":"code","source":"\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom datasets import load_dataset, DatasetDict, Dataset\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\n# from sklearn.preprocessing import StandardScaler  # zvažoval som škálovanie dĺžky komentárov ako vstup\nimport random\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f5ea2c90","cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer\n\n#Najprv vytvorenie Hugging Face Dataset objektov\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# inicializácia tokenizera\ncheckpoint = \"unitary/toxic-bert\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Tokenizačná funkcia\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\n# Tokenizácia\ntrain_ds = train_ds.map(tokenize_function, batched=True)\nval_ds = val_ds.map(tokenize_function, batched=True)\ntest_ds = test_ds.map(tokenize_function, batched=True)\n\n#Premenovanie stĺpca toxicity_label na label potrebuje to Trainer\ntrain_ds = train_ds.rename_column(\"toxicity_label\", \"label\")\nval_ds = val_ds.rename_column(\"toxicity_label\", \"label\")\ntest_ds = test_ds.rename_column(\"toxicity_label\", \"label\")\n\n#Nastavenie formátu pre PyTorch\ntrain_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3c37f756-45f6-4c6e-ac01-dd6cc653bf77","cell_type":"markdown","source":"# 3. Tréning a vyhodnotenie","metadata":{}},{"id":"93629a80","cell_type":"code","source":"!pip install evaluate\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\nimport evaluate\nimport numpy as np\n\n# Nastavenie checkpointu\ncheckpoint = \"unitary/toxic-bert\"\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# pre binárnu klasifikáciu\nconfig = AutoConfig.from_pretrained(checkpoint)\nconfig.num_labels = 2\nconfig.problem_type = \"single_label_classification\"\n\n# Model s upravenou konfiguráciou a automatickou zmenou výstupnej vrstvy\ntoxicbert_model = AutoModelForSequenceClassification.from_pretrained(\n    checkpoint,\n    config=config,\n    ignore_mismatched_sizes=True\n).to(device)\n\n# Metriky\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predicted_labels = np.argmax(logits, axis=-1)\n    return {\n        \"accuracy\": accuracy.compute(predictions=predicted_labels, references=labels)[\"accuracy\"],\n        \"f1\": f1.compute(predictions=predicted_labels, references=labels, average=\"macro\")[\"f1\"]\n    }\n\n# Tréningové argumenty – upravené proti overfittingu\ntraining_args = TrainingArguments(\n    results_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    per_device_train_batch_size=16, # \n    per_device_eval_batch_size=64,\n    num_train_epochs=5,  # znížené z 10 na 5\n    learning_rate=1e-5,  # znížené z 2e-5 na 1e-5\n    weight_decay=0.01,\n    label_smoothing_factor=0.1,  #pridané\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    report_to=\"none\"\n)\n\n\n# Tréner s early stopping\ntrainer = Trainer(\n    toxicbert_model=toxicbert_model,\n    args=training_args,\n    train_dataset=train_ds,     # treba sa uistiť že train_ds je pripravený\n    eval_dataset=val_ds,        # aj tu val_ds je pripravený\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n# Tréning\ntrain_result = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1c7f3ef2","cell_type":"code","source":"\n# ZLEPŠENÉ VYHODNOTENIE \n\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, matthews_corrcoef, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport numpy as np\nimport os\n\n\n\nresults_dir= \"./results\"\nos.makedirs(results_dir, exist_ok=True)\n\n# Predikcie na test\npreds_output = trainer.predict(test_ds)\nlabels = preds_output.label_ids\nfinal_preds = np.argmax(preds_output.predictions, axis=1)\nprobs = torch.nn.functional.softmax(torch.tensor(preds_output.predictions), dim=1)[:, 1].numpy()\n\n# Classification Report\nreport = classification_report(labels, final_preds, target_names=[\"Non-toxic\", \"Toxic\"], digits=4, output_dict=True)\npd.DataFrame(report).transpose().to_csv(f\"{results_dir}/classification_report_full.csv\")\n\nwith open(f\"{results_dir}/classification_report.txt\", \"w\") as f:\n    f.write(classification_report(labels, final_preds, target_names=[\"Non-toxic\", \"Toxic\"], digits=4))\n\n\n\n#Matthews Correlation Coefficient\nmcc = matthews_corrcoef(labels, final_preds)\nprint(\"Matthews Correlation Coefficient (MCC):\", round(mcc, 4))\n\n#Confusion Matrix\ncm = confusion_matrix(labels, final_preds)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"Non-toxic\", \"Toxic\"],\n            yticklabels=[\"Non-toxic\", \"Toxic\"])\nplt.title(\"Confusion Matrix – Test Set\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.tight_layout()\nplt.savefig(f\"{results_dir}/confusion_matrix.png\", dpi=300)\nplt.show()\n\n\n\n# ROc\nfpr, tpr, _ = roc_curve(labels, probs)\nroc_auc = roc_auc_score(labels, probs)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.4f}\", color=\"darkorange\")\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f\"{results_dir}/roc_curve.png\", dpi=300)\nplt.show()              \n\n# Precision-Recall \nprecision, recall, _ = precision_recall_curve(labels, probs)\npr_auc = auc(recall, precision)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f\"PR-AUC = {pr_auc:.4f}\", color=\"blue\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")                        \nplt.title(\"Precision-Recall Curve\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig(f\"{results_dir}/precision_recall_curve.png\", dpi=300)\nplt.show()\n\n#Zlepšené grafy \nepoch_logs = [x for x in trainer.state.log_history if \"epoch\" in x and \"eval_loss\" in x]\n\n\ndef smooth(values, alpha=0.3): # funkcia smooth() vyhladí výkyvy \n    smoothed = []\n    last = values[0]\n    for v in values:\n        smoothed_val = alpha * v + (1 - alpha) * last                              \n        smoothed.append(smoothed_val)\n        last = smoothed_val\n    return smoothed\n\ntrain_loss_epoch = [x[\"loss\"] for x in trainer.state.log_history if \"loss\" in x and \"epoch\" in x]\nval_loss_epoch = [x[\"eval_loss\"] for x in epoch_logs] # train_loss_epoch obsahuje stratu počas učenia val_loss_epoch sleduje chybu na validačných dátach.\nacc_epoch = [x[\"eval_accuracy\"] for x in epoch_logs] # acc_epoch - presnosť klasifikácie\nepochs = [x[\"epoch\"] for x in epoch_logs]\n\n# Loss \nplt.figure(figsize=(10, 6))\nplt.plot(epochs, smooth(train_loss_epoch), label=\"Train Loss\", color=\"blue\", linewidth=2)\nplt.plot(epochs, smooth(val_loss_epoch), label=\"Validation Loss\", color=\"orange\", linewidth=2)\nplt.xlabel(\"Epochs\", fontsize=14)\nplt.ylabel(\"Loss\", fontsize=14)\nplt.title(\"Training and Validation Loss\", fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f\"{results_dir}/loss_clean_graph.png\", dpi=300)\nplt.show()\n\n# Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, smooth(acc_epoch), label=\"Validation Accuracy\", marker='o', color=\"green\", linewidth=2)\nplt.xlabel(\"Epochs\", fontsize=14)\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.title(\"Model Accuracy Over Epochs\", fontsize=16)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.savefig(f\"{results_dir}/accuracy_graph.png\", dpi=300)\nplt.show()\n\n#Chybová analýza\ntry:\n    test_texts = test_ds[\"text\"]\n    errors = [(i, p, l, prob) for i, (p, l, prob) in enumerate(zip(final_preds, labels, probs)) if p != l]\n    errors_sorted = sorted(errors, key=lambda x: abs(x[3] - 0.5))[:5]\n\n    for idx, pred, label, prob in errors_sorted:\n        print(f\"Text: {test_texts[idx][:100]}...\")\n        print(f\"Predicted: {pred}, Actual: {label}, Confidence: {prob:.4f}\")\n        print(\"-\" * 80)\nexcept:\n    print(\"Chybová analýza: Texty nie sú dostupné.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f71052f1","cell_type":"code","source":"\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# mikro, makro a weighted metríky\nf1_micro = f1_score(labels, final_preds, average='micro')\nf1_macro = f1_score(labels, final_preds, average='macro')\nf1_weighted = f1_score(labels, final_preds, average='weighted')\n\nprecision_micro = precision_score(labels, final_preds, average='micro')\nprecision_macro = precision_score(labels, final_preds, average='macro')\nprecision_weighted = precision_score(labels, final_preds, average='weighted')\n\nrecall_micro = recall_score(labels, final_preds, average='micro')\nrecall_macro = recall_score(labels, final_preds, average='macro')\nrecall_weighted = recall_score(labels, final_preds, average='weighted')\n\nprint(f\"F1 micro: {f1_micro:.4f}\")\nprint(f\"F1 macro: {f1_macro:.4f}\")\nprint(f\"F1 weighted: {f1_weighted:.4f}\")\nprint(f\"Precision micro: {precision_micro:.4f}\")\nprint(f\"Precision macro: {precision_macro:.4f}\")\nprint(f\"Precision weighted: {precision_weighted:.4f}\")\nprint(f\"Recall micro: {recall_micro:.4f}\")\nprint(f\"Recall macro: {recall_macro:.4f}\")\nprint(f\"Recall weighted: {recall_weighted:.4f}\")\n\n# Uloženie do csv pre bakalárku\nimport pandas as pd\nmetrics_summary = pd.DataFrame({\n    \"F1\": [f1_micro, f1_macro, f1_weighted],\n    \"Precision\": [precision_micro, precision_macro, precision_weighted],\n    \"Recall\": [recall_micro, recall_macro, recall_weighted]\n}, index=[\"micro\", \"macro\", \"weighted\"])\n\nmetrics_summary.to_csv(f\"{results_dir}/micro_macro_metrics.csv\", float_format=\"%.4f\")\nmetrics_summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff892368-58d1-4c95-992c-b6eae71375fc","cell_type":"code","source":"import zipfile\nimport os\n\noutput_zip = \"toxicbert_bin_results.zip\"\nresults_dir = \"results\"\n\nwith zipfile.ZipFile(output_zip, \"w\") as zipf:\n    for file in os.listdir(results_dir):\n        if file.endswith(\".png\") or file.endswith(\".csv\") or file.endswith(\".txt\"):\n            zipf.write(os.path.join(results_dir, file), arcname=file)\n\nprint(f\"ZIP súbor vytvorený: {output_zip}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}